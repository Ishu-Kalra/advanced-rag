{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages (ensure these are installed in your environment)\n",
    "# %pip install -qU pypdf==4.0.1 langchain_community\n",
    "# %pip install -U duckduckgo-search\n",
    "# %pip install -qU langchain-openai\n",
    "# %pip install faiss-cpu\n",
    "# %pip install langchain==0.3.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U sentence-transformers\n",
    "# %pip install -U torch  # For CPU\n",
    "# If you have a GPU and want to leverage it, install the appropriate PyTorch version:\n",
    "# pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import DuckDuckGoSearchResults, StructuredTool, Tool\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import DuckDuckGoSearchResults, StructuredTool, Tool\n",
    "from langchain.chat_models import ChatOpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import langchain\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg[binary,pool] in /opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages (3.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages (from psycopg[binary,pool]) (4.12.2)\n",
      "Requirement already satisfied: psycopg-binary==3.2.3 in /opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages (from psycopg[binary,pool]) (3.2.3)\n",
      "Requirement already satisfied: psycopg-pool in /opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages (from psycopg[binary,pool]) (3.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"psycopg[binary,pool]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "connection_string = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "# db = PGVector.from_documents(documents, model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securely set your OpenAI API key as an environment variable before running the script\n",
    "# Example (in your terminal):\n",
    "# export OPENAI_API_KEY=\"your-api-key\"\n",
    "\n",
    "import os\n",
    "api_key = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"API_KEY_OPENAI\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of company names and their corresponding PDF file paths\n",
    "\n",
    "companies = {\n",
    "    \"Nike\": \"../../data/Nike-Annual-Report.pdf\",\n",
    "    \"Lyft\": \"../../data/Lyft-Annual-Report-2021.pdf\",\n",
    "    \"Uber\": \"../../data/Uber-Annual-Report-2021.pdf\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to hold data for each company\n",
    "\n",
    "company_docs = {}\n",
    "company_vectorstores = {}\n",
    "company_retrievers = {}\n",
    "company_qa_tools = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Create embeddings\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# llm = ChatOpenAI()  # Ensure you have access to GPT-4\n",
    "llm = Ollama(model=\"llama3\", base_url=\"http://localhost:11434/\")\n",
    "\n",
    "# Process each company's PDF\n",
    "for company, file_path in companies.items():\n",
    "    # Load the PDF document\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Split the document into chunks\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Create a PGVector vector store for the company's documents\n",
    "    vectorstore = PGVector.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,  # Correct parameter name\n",
    "    connection=connection_string,\n",
    "    collection_name=f\"{company.lower()}_documents\"  # Separate table for each company\n",
    "    )\n",
    "    \n",
    "    # Create a retriever for the vector store\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # Store in dictionaries\n",
    "    company_docs[company] = docs\n",
    "    company_vectorstores[company] = vectorstore\n",
    "    company_retrievers[company] = retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/w5nd7ynn3n76b25r6j68r0g40000gn/T/ipykernel_16390/3717966962.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3\", base_url=\"http://localhost:11434/\")\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "\n",
    "# Define the QAInput model\n",
    "class QAInput(BaseModel):\n",
    "    query: str = Field(..., description=\"The query to ask the company's documents.\")\n",
    "\n",
    "# Define a function that extracts the query from QAInput and runs the QA chain\n",
    "def qa_run(input: QAInput, **kwargs) -> str:\n",
    "    return qa_chain.run(input.query)\n",
    "\n",
    "# Create StructuredTool instances for each company\n",
    "qa_tools = []\n",
    "for company, retriever in company_retrievers.items():\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # You can choose 'stuff', 'map_reduce', etc.\n",
    "        retriever=retriever\n",
    "    )\n",
    "    \n",
    "    qa_tool = StructuredTool.from_function(\n",
    "        func=qa_run,  # Use the wrapper function\n",
    "        name=f\"{company} QA\",\n",
    "        description=f\"Use this tool to answer questions about {company}'s information, especially financial information.\",\n",
    "        args_schema=QAInput  # Use the concrete Pydantic model\n",
    "    )\n",
    "    qa_tools.append(qa_tool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Revenue Growth Calculator Tool\n",
    "class RevenueGrowthInput(BaseModel):\n",
    "    revenue_new: float = Field(..., description=\"Revenue in the new year\")\n",
    "    revenue_old: float = Field(..., description=\"Revenue in the old year\")\n",
    "\n",
    "def calculate_revenue_growth(revenue_new: float, revenue_old: float) -> float:\n",
    "    \"\"\"Calculates revenue growth from the old year to the new year.\"\"\"\n",
    "    return ((revenue_new - revenue_old) / revenue_old) * 100\n",
    "\n",
    "calculate_growth_tool = StructuredTool.from_function(\n",
    "    func=calculate_revenue_growth,\n",
    "    name=\"Revenue Growth Calculator\",\n",
    "    description=(\n",
    "        \"Calculates the revenue growth percentage between two years. \"\n",
    "        \"Requires 'revenue_new' and 'revenue_old' as inputs.\"\n",
    "    ),\n",
    "    args_schema=RevenueGrowthInput\n",
    ")\n",
    "\n",
    "# Define the DuckDuckGo search tool\n",
    "web_search_tool = DuckDuckGoSearchResults(\n",
    "    name=\"Web Search\",\n",
    "    description=(\n",
    "        \"Use this tool to search the web for financial information such as revenue figures \"\n",
    "        \"if they are not found in the documents.\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of company names for routing\n",
    "company_names = list(companies.keys())\n",
    "\n",
    "# Create embeddings for routing (reuse the existing embeddings)\n",
    "routing_embeddings = embeddings  # Using the same embedding model\n",
    "\n",
    "# Create a PGVector vector store for routing\n",
    "routing_vectorstore = PGVector.from_texts(\n",
    "    texts=company_names,\n",
    "    embedding=embeddings,  # Correct parameter name\n",
    "    connection=connection_string,\n",
    "    collection_name=\"routing\"  # Correct parameter name\n",
    ")\n",
    "\n",
    "# Define a function to get the most similar company based on query\n",
    "def get_most_similar_company(query: str) -> Optional[str]:\n",
    "    similar_docs = routing_vectorstore.similarity_search(query, k=1)\n",
    "    if similar_docs:\n",
    "        return similar_docs[0].page_content\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define the semantic routing function\n",
    "def semantic_router(query: str) -> str:\n",
    "    company = get_most_similar_company(query)\n",
    "    if company and company in companies:\n",
    "        return company\n",
    "    else:\n",
    "        return \"Web Search\"  # Fallback to web search if no company matches\n",
    "\n",
    "# Define the Semantic Router Tool\n",
    "router_tool = Tool(\n",
    "    name=\"Semantic Router\",\n",
    "    func=semantic_router,\n",
    "    description=(\n",
    "        \"Determines which company's knowledge base to use for answering the question. \"\n",
    "        \"Returns the company name (Nike, Lyft, Uber) or 'Web Search' if no match is found.\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/w5nd7ynn3n76b25r6j68r0g40000gn/T/ipykernel_16390/3302426359.py:7: LangChainPendingDeprecationWarning: This class is deprecated and will be removed in a future version. You can swap to using the `PostgresChatMessageHistory` implementation in `langchain_postgres`. Please do not submit further PRs to this class.See <https://github.com/langchain-ai/langchain-postgres>\n",
      "  history = PostgresChatMessageHistory(\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.chat_message_histories import (\n",
    "#     PostgresChatMessageHistory,\n",
    "# )\n",
    "# import uuid\n",
    "# session_id = str(uuid.uuid4())\n",
    "\n",
    "# history = PostgresChatMessageHistory(\n",
    "#     connection_string='postgresql://langchain:langchain@localhost:6024/langchain',\n",
    "#     session_id=session_id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.chat_message_histories.postgres.PostgresChatMessageHistory at 0x105b94e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory.vectorstore import VectorStoreRetrieverMemory\n",
    "\n",
    "# # Initialize PGVector for memory\n",
    "# memory_vectorstore = PGVector.from_documents(\n",
    "#     # documents=[],  # Initialize with empty documents\n",
    "#     embedding=embeddings,  # Correct parameter name\n",
    "#     connection=connection_string,\n",
    "#     collection_name=\"memory\"  # Correct parameter name\n",
    "# )\n",
    "\n",
    "# # Initialize VectorStoreRetrieverMemory\n",
    "# memory = VectorStoreRetrieverMemory(\n",
    "#     retriever=memory_vectorstore.as_retriever(),\n",
    "#     memory_key=\"chat_history\",\n",
    "#     return_docs=False  # Set to True if you want to return documents in memory\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update the tools list to include the router and individual QA tools\n",
    "tools = [router_tool] + qa_tools + [calculate_growth_tool, web_search_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the system message\n",
    "# %%\n",
    "# Define the system message with clear instructions and examples\n",
    "system_message = \"\"\"\n",
    "You are an assistant that helps answer financial questions about companies.\n",
    "\n",
    "When given a question, you should:\n",
    "\n",
    "1. Use the 'Semantic Router' tool to determine which company's information is relevant to the question (Nike, Lyft, or Uber).\n",
    "2. Use the appropriate '{company} QA' tool to find information from the selected company's documents.\n",
    "3. If information is missing, use the 'Web Search' tool to find it.\n",
    "4. Once you have all the necessary information, if you need to do some calculation regarding revenue growth, use the 'Revenue Growth Calculator' tool to compute or provide the answer.\n",
    "\n",
    "**Important:** When deciding to use a tool, output your response in the following JSON format **exactly**:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"action\": \"Tool Name\",\n",
    "  \"action_input\": \"Input for the tool\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_kwargs = {\n",
    "    \"system_message\": system_message\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\",return_messages=True,k=7)\n",
    "\n",
    "# Initialize the agent with the updated tools and system message\n",
    "agent_chain = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    agent_kwargs=agent_kwargs,\n",
    "    memory = memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/w5nd7ynn3n76b25r6j68r0g40000gn/T/ipykernel_16390/3247499933.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent_chain.run(\"Hello\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The human has initiated conversation, I should respond accordingly.\n",
      "\n",
      "Action:\n",
      "{\n",
      "  \"action\": \"Semantic Router\",\n",
      "  \"action_input\": {\"type\": 'string'}\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thought: The human has initiated conversation, I should respond accordingly.\\n\\nAction:\\n{\\n  \"action\": \"Semantic Router\",\\n  \"action_input\": {\"type\": \\'string\\'}\\n}\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "{\n",
      "  \"action\": \"Semantic Router\",\n",
      "  \"action_input\": {\"type\": 'string'}\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Action:\\n{\\n  \"action\": \"Semantic Router\",\\n  \"action_input\": {\"type\": \\'string\\'}\\n}\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\"What query I asked you before?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test queries\n",
    "queries = [\n",
    "    \"What is the revenue growth between 2022 and 2023 for Lyft?\",\n",
    "    \"Summarize Uber's financial performance in 2022.\",\n",
    "    \"What are Nike's net profits for the last quarter?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = agent_chain.run(query)\n",
    "    print(f\"Answer: {results}\")\n",
    "\n",
    "# Additional test queries\n",
    "additional_queries = [\n",
    "    \"Summarize the Lyft document. What info does it mention?\",\n",
    "    \"What is Nike's net profit after the end of May 31, 2023?\"\n",
    "]\n",
    "\n",
    "for query in additional_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = agent_chain.run(query)\n",
    "    print(f\"Answer: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_recursive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
