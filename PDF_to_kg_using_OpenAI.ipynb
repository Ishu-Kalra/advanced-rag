{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb375284",
   "metadata": {},
   "source": [
    "# Creating Knowledge graphs from Pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b2dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd3b306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-7b.\n401 Client Error. (Request ID: Root=1-670fc297-3496578037ee00c4171f6528;a3640e26-020f-4726-9e75-77e490ab600b)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/config.json.\nAccess to model google/gemma-7b is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/google/gemma-7b/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/file_download.py:1339\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1339\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/file_download.py:1746\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1746\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/file_download.py:1666\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1666\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1675\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/file_download.py:364\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 364\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/file_download.py:388\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    387\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 388\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:423\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    420\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m     )\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-670fc297-3496578037ee00c4171f6528;a3640e26-020f-4726-9e75-77e490ab600b)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/config.json.\nAccess to model google/gemma-7b is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Initialize model configuration\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# BnB Configuration\u001b[39;00m\n\u001b[1;32m     68\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     69\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     71\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mbfloat16\n\u001b[1;32m     73\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:526\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 526\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1006\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1004\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1006\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1008\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/transformers/configuration_utils.py:570\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/transformers/configuration_utils.py:629\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-7b.\n401 Client Error. (Request ID: Root=1-670fc297-3496578037ee00c4171f6528;a3640e26-020f-4726-9e75-77e490ab600b)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/config.json.\nAccess to model google/gemma-7b is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# **1. Import Necessary Libraries**\n",
    "# =============================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    pipeline,\n",
    ")\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.schema import Document\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.graphs.graph_document import Node, Relationship, GraphDocument\n",
    "from neo4j import GraphDatabase\n",
    "import logging\n",
    "\n",
    "# =============================\n",
    "# **2. Setup Logging and Environment Variables**\n",
    "# =============================\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define Neo4j connection parameters\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"password\")  # Update as needed\n",
    "\n",
    "# =============================\n",
    "# **3. Initialize HuggingFace Model and Tokenizer**\n",
    "# =============================\n",
    "\n",
    "model_id = 'google/gemma-7b'\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize model configuration\n",
    "model_config = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# BnB Configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# Load the model with quantization\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=model_config,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# =============================\n",
    "# **4. Define Utility Functions**\n",
    "# =============================\n",
    "\n",
    "def bytes_to_giga_bytes(bytes_val):\n",
    "    return bytes_val / 1024 / 1024 / 1024\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "flush()\n",
    "\n",
    "# Define stopping criteria\n",
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.equal(input_ids[0][-len(stop_ids):], stop_ids):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "# =============================\n",
    "# **5. Setup Text Generation Pipeline**\n",
    "# =============================\n",
    "\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if cuda.is_available() else -1,\n",
    "    temperature=0.3,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1,\n",
    "    stopping_criteria=stopping_criteria,\n",
    ")\n",
    "\n",
    "result = generate_text(\"What are the primary mechanisms underlying antibiotic resistance, and how can we develop strategies to combat it?\")\n",
    "print(result)\n",
    "\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "# =============================\n",
    "# **6. Initialize LLM**\n",
    "# =============================\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# =============================\n",
    "# **7. Load and Process Documents**\n",
    "# =============================\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('/content/drive/MyDrive/BioMedical-Dataset', glob=\"**/*.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=30,\n",
    "    add_start_index=True,\n",
    "    separators=MARKDOWN_SEPARATORS\n",
    ")\n",
    "\n",
    "processed_text_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of text splits: {len(processed_text_splits)}\")\n",
    "\n",
    "# =============================\n",
    "# **8. Generate Embeddings**\n",
    "# =============================\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# **9. Initialize Neo4j Graph**\n",
    "# =============================\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# **10. Create Vector Index in Neo4j**\n",
    "# =============================\n",
    "\n",
    "# Assuming the vector index is already created in Neo4j\n",
    "# If not, uncomment and run the following code once\n",
    "'''\n",
    "graph.query(\"\"\"\n",
    "CALL db.index.vector.createNodeIndex(\n",
    "  'KG-Enhanced-QnA-Biomedical',\n",
    "  'text_splits',\n",
    "  'embeddings',\n",
    "   768,\n",
    "   'cosine'\n",
    ")\n",
    "\"\"\")\n",
    "'''\n",
    "\n",
    "# =============================\n",
    "# **11. Display Created Vector Indexes**\n",
    "# =============================\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "def show_vector_indexes(uri, username, password):\n",
    "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"SHOW VECTOR INDEXES\")\n",
    "        for record in result:\n",
    "            print(record)\n",
    "    driver.close()\n",
    "\n",
    "show_vector_indexes(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "# =============================\n",
    "# **12. Insert Embeddings into Neo4j**\n",
    "# =============================\n",
    "\n",
    "# Define the data to be inserted\n",
    "chunks = [\n",
    "    {\n",
    "        'text': document.page_content,\n",
    "        'embedding': embeddings.embed_query(document.page_content)\n",
    "    }\n",
    "    for document in processed_text_splits if len(document.page_content) > 50\n",
    "]\n",
    "\n",
    "# Insert data into Neo4j\n",
    "def insert_embeddings_into_neo4j(chunks, graph):\n",
    "    query = \"\"\"\n",
    "    UNWIND $data AS row\n",
    "    CREATE (c:Chunk {text: row.text})\n",
    "    WITH c, row\n",
    "    CALL db.create.setVectorProperty(c, 'embedding', row.embedding)\n",
    "    YIELD node\n",
    "    RETURN distinct 'done'\n",
    "    \"\"\"\n",
    "    graph.query(query, {'data': chunks})\n",
    "\n",
    "# Uncomment to insert embeddings\n",
    "# insert_embeddings_into_neo4j(chunks, graph)\n",
    "\n",
    "# =============================\n",
    "# **13. Initialize Neo4jVector**\n",
    "# =============================\n",
    "\n",
    "neo4j_vector = Neo4jVector.from_documents(\n",
    "    processed_text_splits,\n",
    "    embeddings,\n",
    "    index_name='KG-Enhanced-QnA-Biomedical',\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# **14. Perform Similarity Search**\n",
    "# =============================\n",
    "\n",
    "query = \"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\"\n",
    "vector_results = neo4j_vector.similarity_search(query, k=2)\n",
    "\n",
    "for i, res in enumerate(vector_results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(res.page_content)\n",
    "    if i != len(vector_results) - 1:\n",
    "        print()\n",
    "\n",
    "vector_result = vector_results[0].page_content\n",
    "\n",
    "# =============================\n",
    "# **15. Define Custom Neo4jVectorChain**\n",
    "# =============================\n",
    "\n",
    "class Neo4jVectorChain(Chain):\n",
    "    graph: Neo4jGraph = Field(exclude=True)\n",
    "    input_key: str = \"query\"\n",
    "    output_key: str = \"result\"\n",
    "    embeddings: HuggingFaceBgeEmbeddings = HuggingFaceBgeEmbeddings()\n",
    "    qa_chain: LLMChain = LLMChain(llm=llm, prompt=CHAT_PROMPT)\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [self.input_key]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str], run_manager, k=3) -> Dict[str, Any]:\n",
    "        question = inputs[self.input_key]\n",
    "        embedding = self.embeddings.embed_query(question)\n",
    "\n",
    "        context = self.graph.query(vector_search, {'embedding': embedding, 'k': k})\n",
    "        context = [el['result'] for el in context]\n",
    "\n",
    "        result = self.qa_chain({\"question\": question, \"context\": context})\n",
    "        final_result = result[self.qa_chain.output_key]\n",
    "        return {self.output_key: final_result}\n",
    "\n",
    "# Define the Cypher query for vector search\n",
    "vector_search = \"\"\"\n",
    "WITH $embedding AS e\n",
    "CALL db.index.vector.queryNodes('KG-Enhanced-QnA-Biomedical', $k, e) YIELD node, score\n",
    "RETURN node.text AS result\n",
    "ORDER BY score DESC\n",
    "LIMIT $k\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the custom Neo4jVectorChain\n",
    "class Neo4jVectorChainCustom(Chain):\n",
    "    graph: Neo4jGraph\n",
    "    embeddings: HuggingFaceBgeEmbeddings\n",
    "    qa_chain: LLMChain\n",
    "    input_key: str = \"query\"\n",
    "    output_key: str = \"result\"\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [self.input_key]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str], run_manager=None) -> Dict[str, Any]:\n",
    "        question = inputs[self.input_key]\n",
    "        embedding = self.embeddings.embed_query(question)\n",
    "\n",
    "        context = self.graph.query(\n",
    "            vector_search,\n",
    "            {'embedding': embedding, 'k': 3}\n",
    "        )\n",
    "        context = [el['result'] for el in context]\n",
    "\n",
    "        result = self.qa_chain({\"question\": question, \"context\": context})\n",
    "        final_result = result[self.qa_chain.output_key]\n",
    "        return {self.output_key: final_result}\n",
    "\n",
    "# Instantiate the custom chain\n",
    "chain = Neo4jVectorChainCustom(\n",
    "    graph=graph,\n",
    "    embeddings=embeddings,\n",
    "    qa_chain=LLMChain(llm=llm, prompt=CHAT_PROMPT),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# **16. Run the Custom Chain for QA**\n",
    "# =============================\n",
    "\n",
    "graph_result = chain.run(\n",
    "    \"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\"\n",
    ")\n",
    "\n",
    "print(\"QA Result:\")\n",
    "print(graph_result)\n",
    "\n",
    "# =============================\n",
    "# **17. Alternative: Using GraphCypherQAChain**\n",
    "# =============================\n",
    "\n",
    "chain_alternative = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=llm,\n",
    "    qa_llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    validate_cypher=True\n",
    ")\n",
    "\n",
    "graph_result_alternative = chain_alternative.run(\n",
    "    \"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\"\n",
    ")\n",
    "\n",
    "print(\"Alternative QA Result:\")\n",
    "print(graph_result_alternative)\n",
    "\n",
    "# =============================\n",
    "# **18. Clean Up Neo4j Driver**\n",
    "# =============================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874b7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "# neo4j_uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')\n",
    "# neo4j_user = os.environ.get('NEO4J_USER', 'neo4j')\n",
    "# neo4j_password = os.environ.get('NEO4J_PASSWORD', 'password')\n",
    "\n",
    "# url = \"bolt://localhost:7687\"\n",
    "# username =\"neo4j\"\n",
    "# password = \"password\"\n",
    "# graph = Neo4jGraph(\n",
    "#     url=url,\n",
    "#     username=username,\n",
    "#     password=password\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e7e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Add OpenAI key\n",
    "import os\n",
    "api_key = \"sk-proj-nFVOfsf7i5TkjCVt2OciT3BlbkFJItRqIcprNW7DWSSLP83u\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"API_KEY_OPENAI\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f88eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.graphs.graph_document import GraphDocument\n",
    "from langchain.schema import Document\n",
    "from typing import List, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Set environment variables with the correct password\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USER\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"  # Updated to the correct password\n",
    "\n",
    "class Property(BaseModel):\n",
    "    \"\"\"A single property consisting of key and value\"\"\"\n",
    "    key: str = Field(..., description=\"key\")\n",
    "    value: str = Field(..., description=\"value\")\n",
    "\n",
    "# class Node(BaseNode, BaseModel):\n",
    "#     properties: Optional[List[Property]] = Field(\n",
    "#         None, description=\"List of node properties\"\n",
    "#     )\n",
    "\n",
    "# class Relationship(BaseRelationship, BaseModel):\n",
    "#     properties: Optional[List[Property]] = Field(\n",
    "#         None, description=\"List of relationship properties\"\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e77636d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Mapping Node: id='john_doe' type='Person' properties={'age': '30', 'birthDate': '1990-01-01'}\n",
      "INFO:__main__:Inserting node: john_doe of type Person\n",
      "INFO:__main__:Mapping Node: id='acme_corp' type='Organization' properties={'industry': 'Technology'}\n",
      "INFO:__main__:Inserting node: acme_corp of type Organization\n",
      "INFO:__main__:Mapping Relationship: source=Node(id='john_doe', type='Person', properties={'age': '30', 'birthDate': '1990-01-01'}) target=Node(id='acme_corp', type='Organization', properties={'industry': 'Technology'}) type='WORKS_AT' properties={'since': '2015'}\n",
      "INFO:__main__:Mapping Node: id='john_doe' type='Person' properties={'age': '30', 'birthDate': '1990-01-01'}\n",
      "INFO:__main__:Mapping Node: id='acme_corp' type='Organization' properties={'industry': 'Technology'}\n",
      "INFO:__main__:Inserting relationship: WORKS_AT between john_doe and acme_corp\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (b))} {position: line: 1, column: 1, offset: 0} for query: 'MATCH (a:`Person` {id: $source_id}), (b:`Organization` {id: $target_id}) MERGE (a)-[r:`WORKS_AT`]->(b) SET r += $properties RETURN r'\n",
      "INFO:__main__:Knowledge graph inserted into Neo4j successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.graphs.graph_document import Node, Relationship, GraphDocument\n",
    "from langchain.schema import Document\n",
    "from typing import Dict, Any\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define Neo4j connection parameters\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USER\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"password\")  # Update as needed\n",
    "\n",
    "def get_neo4j_driver():\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    return driver\n",
    "\n",
    "driver = get_neo4j_driver()\n",
    "\n",
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def map_to_node_data(node: Node) -> Dict[str, Any]:\n",
    "    \"\"\"Map the KnowledgeGraph Node to a dictionary suitable for Neo4j.\"\"\"\n",
    "    logger.info(f\"Mapping Node: {node}\")\n",
    "    if not hasattr(node, 'id') or not hasattr(node, 'type'):\n",
    "        raise AttributeError(\"Node is missing 'id' or 'type' attributes\")\n",
    "    if not isinstance(node.id, (str, int)) or not isinstance(node.type, str):\n",
    "        raise TypeError(\"'id' must be str or int and 'type' must be a string\")\n",
    "    \n",
    "    properties = node.properties.copy() if node.properties else {}\n",
    "    # Optionally add a 'name' property if needed\n",
    "    if isinstance(node.id, str):\n",
    "        properties[\"name\"] = node.id.title()\n",
    "    return {\n",
    "        \"id\": node.id,\n",
    "        \"type\": node.type.capitalize(),\n",
    "        \"properties\": properties\n",
    "    }\n",
    "\n",
    "def map_to_relationship_data(rel: Relationship) -> Dict[str, Any]:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to a dictionary suitable for Neo4j.\"\"\"\n",
    "    logger.info(f\"Mapping Relationship: {rel}\")\n",
    "    if not hasattr(rel, 'type') or not hasattr(rel, 'source') or not hasattr(rel, 'target'):\n",
    "        raise AttributeError(\"Relationship is missing 'type', 'source', or 'target' attributes\")\n",
    "    if not isinstance(rel.type, str):\n",
    "        raise TypeError(\"'type' attribute must be a string\")\n",
    "    if not isinstance(rel.source, Node) or not isinstance(rel.target, Node):\n",
    "        raise TypeError(\"'source' and 'target' must be instances of Node\")\n",
    "    \n",
    "    source_data = map_to_node_data(rel.source)\n",
    "    target_data = map_to_node_data(rel.target)\n",
    "    properties = rel.properties.copy() if rel.properties else {}\n",
    "    return {\n",
    "        \"source\": source_data,\n",
    "        \"target\": target_data,\n",
    "        \"type\": rel.type,\n",
    "        \"properties\": properties\n",
    "    }\n",
    "\n",
    "def create_node(tx, node_data):\n",
    "    logger.info(f\"Inserting node: {node_data['id']} of type {node_data['type']}\")\n",
    "    # Inject the label directly using f-strings\n",
    "    query = (\n",
    "        f\"MERGE (n:`{node_data['type']}` {{id: $id}}) \"\n",
    "        \"SET n += $properties \"\n",
    "        \"RETURN n\"\n",
    "    )\n",
    "    result = tx.run(query, id=node_data[\"id\"], properties=node_data[\"properties\"])\n",
    "    return result.single()\n",
    "\n",
    "def create_relationship(tx, rel_data):\n",
    "    logger.info(f\"Inserting relationship: {rel_data['type']} between {rel_data['source']['id']} and {rel_data['target']['id']}\")\n",
    "    # Inject the relationship type directly using f-strings\n",
    "    query = (\n",
    "        f\"MATCH (a:`{rel_data['source']['type']}` {{id: $source_id}}), \"\n",
    "        f\"(b:`{rel_data['target']['type']}` {{id: $target_id}}) \"\n",
    "        f\"MERGE (a)-[r:`{rel_data['type']}`]->(b) \"\n",
    "        \"SET r += $properties \"\n",
    "        \"RETURN r\"\n",
    "    )\n",
    "    result = tx.run(\n",
    "        query,\n",
    "        source_id=rel_data[\"source\"][\"id\"],\n",
    "        target_id=rel_data[\"target\"][\"id\"],\n",
    "        properties=rel_data[\"properties\"]\n",
    "    )\n",
    "    return result.single()\n",
    "\n",
    "def insert_knowledge_graph(knowledge_graph: GraphDocument, driver):\n",
    "    with driver.session() as session:\n",
    "        # Insert all nodes\n",
    "        for node in knowledge_graph.nodes:\n",
    "            node_data = map_to_node_data(node)\n",
    "            session.execute_write(create_node, node_data)\n",
    "        \n",
    "        # Insert all relationships\n",
    "        for rel in knowledge_graph.relationships:\n",
    "            rel_data = map_to_relationship_data(rel)\n",
    "            session.execute_write(create_relationship, rel_data)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your nodes\n",
    "    node1 = Node(\n",
    "        id=\"john_doe\",\n",
    "        type=\"Person\",\n",
    "        properties={\n",
    "            \"age\": \"30\",\n",
    "            \"birthDate\": \"1990-01-01\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    node2 = Node(\n",
    "        id=\"acme_corp\",\n",
    "        type=\"Organization\",\n",
    "        properties={\n",
    "            \"industry\": \"Technology\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Define a relationship\n",
    "    relationship = Relationship(\n",
    "        source=node1,\n",
    "        target=node2,\n",
    "        type=\"WORKS_AT\",\n",
    "        properties={\n",
    "            \"since\": \"2015\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create GraphDocument\n",
    "    knowledge_graph = GraphDocument(\n",
    "        nodes=[node1, node2],\n",
    "        relationships=[relationship],\n",
    "        source=Document(page_content=\"John Doe works at Acme Corp since 2015.\")\n",
    "    )\n",
    "\n",
    "    # Insert into Neo4j\n",
    "    try:\n",
    "        insert_knowledge_graph(knowledge_graph, driver)\n",
    "        logger.info(\"Knowledge graph inserted into Neo4j successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inserting knowledge graph: {e}\")\n",
    "    finally:\n",
    "        driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6d19f",
   "metadata": {},
   "source": [
    "# Using GPT-3.5-turbo-16k model to create the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505a5db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to write data to connection IPv4Address(('localhost', 7687)) (ResolvedIPv4Address(('127.0.0.1', 7687)))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def get_extraction_chain(\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\n",
    "          \"system\",\n",
    "          f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "## 5. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "          \"\"\"),\n",
    "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "        ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8b15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes:Optional[List[str]] = None,\n",
    "    rels:Optional[List[str]]=None) -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(nodes, rels)\n",
    "    data = extract_chain.invoke(document.page_content)['function']\n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "      source = document\n",
    "    )\n",
    "    # Store information into a graph\n",
    "    print(graph_document)\n",
    "    graph.add_graph_documents([graph_document])\n",
    "    return graph_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7552cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File path ../data/Football_news.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m----> 7\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/Football_news.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     11\u001b[0m pages \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_and_split()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:241\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with `pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(\n\u001b[1;32m    243\u001b[0m     password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[1;32m    244\u001b[0m     extract_images\u001b[38;5;241m=\u001b[39mextract_images,\n\u001b[1;32m    245\u001b[0m     extraction_mode\u001b[38;5;241m=\u001b[39mextraction_mode,\n\u001b[1;32m    246\u001b[0m     extraction_kwargs\u001b[38;5;241m=\u001b[39mextraction_kwargs,\n\u001b[1;32m    247\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:117\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path ../data/Football_news.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "loader = PyPDFLoader(\"../data/Football_news.pdf\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "# Only take the first 4 pages of the document\n",
    "documents = text_splitter.split_documents(pages[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "249d841a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m distinct_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m      4\u001b[0m relations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[43mdocuments\u001b[49m), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(documents)):\n\u001b[1;32m      7\u001b[0m     graph_document\u001b[38;5;241m=\u001b[39mextract_and_store_graph(d)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#Get distinct nodes\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "distinct_nodes = set()\n",
    "relations = []\n",
    "\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    graph_document=extract_and_store_graph(d)\n",
    "    \n",
    "    #Get distinct nodes\n",
    "    for node in graph_document.nodes :\n",
    "        distinct_nodes.add(node.id)\n",
    "    \n",
    "    #Get all relations   \n",
    "    for relation in graph_document.relationships :\n",
    "        relations.append(relation.type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5d74ba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \n\u001b[1;32m      5\u001b[0m LLM \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI-GPT-3.5-turbo-16k\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241m.\u001b[39mfile_path\n\u001b[1;32m      7\u001b[0m processed_time \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m()\n\u001b[1;32m      9\u001b[0m llm_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m : LLM, \n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m\"\u001b[39m : loader\u001b[38;5;241m.\u001b[39mfile_path, \n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Time\u001b[39m\u001b[38;5;124m\"\u001b[39m : processed_time, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNodes\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28mlist\u001b[39m(distinct_nodes),\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelations\u001b[39m\u001b[38;5;124m\"\u001b[39m : relations }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "end_time = datetime.now() \n",
    "\n",
    "LLM = \"OpenAI-GPT-3.5-turbo-16k\"\n",
    "file = loader.file_path\n",
    "processed_time = (end_time - start_time).__str__()\n",
    "\n",
    "llm_data = {\"LLM\" : LLM, \n",
    "            \"File\" : loader.file_path, \n",
    "            \"Processing Time\" : processed_time, \n",
    "            \"Node count\" : len(distinct_nodes), \n",
    "            \"Relation count\" : len(relations),\n",
    "            \"Nodes\" : list(distinct_nodes),\n",
    "            \"Relations\" : relations }\n",
    "\n",
    "json_file_path = '../data/llm_comparision.json'\n",
    "with open(json_file_path, 'r') as json_file :\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "if isinstance(data, dict) :\n",
    "    data = [data]\n",
    "    \n",
    "data.append(llm_data)   \n",
    "\n",
    "with open(json_file_path, 'w') as json_file :\n",
    "    json.dump(data, json_file, indent=4) \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d488dba",
   "metadata": {},
   "source": [
    "Reference Document: https://bratanic-tomaz.medium.com/constructing-knowledge-graphs-from-text-using-openai-functions-096a6d010c17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0134276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (llama-index-recursive)",
   "language": "python",
   "name": "llama-index-recursive"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
