{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517694d6",
   "metadata": {},
   "source": [
    "# Creating Knowledge graphs from Pdf files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446de94a",
   "metadata": {},
   "source": [
    "Reference Document: https://bratanic-tomaz.medium.com/constructing-knowledge-graphs-from-text-using-openai-functions-096a6d010c17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e075be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents after splitting: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing GraphDocument with 0 nodes and 0 relationships.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:  11%|█         | 1/9 [00:03<00:25,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing GraphDocument with 5 nodes and 2 relationships.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:  33%|███▎      | 3/9 [00:34<01:17, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting and storing graph: 'str' object has no attribute 'value'\n",
      "Error processing document 2: 'str' object has no attribute 'value'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:  44%|████▍     | 4/9 [01:01<01:32, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing GraphDocument with 5 nodes and 4 relationships.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:  56%|█████▌    | 5/9 [01:04<00:51, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing GraphDocument with 1 nodes and 0 relationships.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:  67%|██████▋   | 6/9 [01:25<00:47, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing GraphDocument with 5 nodes and 4 relationships.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:  78%|███████▊  | 7/9 [01:39<00:30, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting and storing graph: 'str' object has no attribute 'value'\n",
      "Error processing document 6: 'str' object has no attribute 'value'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents:  89%|████████▉ | 8/9 [01:49<00:13, 13.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing GraphDocument with 3 nodes and 2 relationships.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 100%|██████████| 9/9 [01:50<00:00, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing GraphDocument with 0 nodes and 0 relationships.\n",
      "Metadata updated successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# **1. Import Necessary Libraries**\n",
    "# =============================\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from functools import partial\n",
    "\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain_community.graphs.graph_document import Node, Relationship, GraphDocument\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# =============================\n",
    "# **2. Setup Environment Variables**\n",
    "# =============================\n",
    "\n",
    "# Load environment variables from .env file (if using one)\n",
    "load_dotenv()\n",
    "\n",
    "# Define Neo4j connection parameters\n",
    "# Set OpenAI API key directly (Note: It's not recommended to hardcode API keys in scripts)\n",
    "# Add OpenAI API key\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"password\")  # Update as needed\n",
    "\n",
    "api_key = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"API_KEY_OPENAI\"] = api_key\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')  # Ensure this is set in your environment\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"Error: OPENAI_API_KEY is not set in the environment variables.\")\n",
    "    raise ValueError(\"OPENAI_API_KEY is required.\")\n",
    "\n",
    "# =============================\n",
    "# **3. Initialize OpenAI Embeddings and LLM**\n",
    "# =============================\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize OpenAI ChatGPT (GPT-4) LLM\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-4',\n",
    "    temperature=0  # Adjust as needed\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# **4. Define Utility Functions**\n",
    "# =============================\n",
    "\n",
    "def flush():\n",
    "    \"\"\"\n",
    "    Perform garbage collection to free up memory.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "flush()\n",
    "\n",
    "# =============================\n",
    "# **5. Define KnowledgeGraph Model and Mapping Functions**\n",
    "# =============================\n",
    "\n",
    "# Define KnowledgeGraph class\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )\n",
    "\n",
    "def format_property_key(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a string to camelCase for property keys.\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props: Optional[List[BaseModel]]) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a list of Property models to a dictionary.\n",
    "    \"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "        return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> Node:\n",
    "    \"\"\"\n",
    "    Map the KnowledgeGraph Node to the base Node with proper formatting.\n",
    "    \"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    if 'name' not in properties and isinstance(node.id, str):\n",
    "        properties[\"name\"] = node.id.title()\n",
    "    return Node(\n",
    "        id=node.id.title() if isinstance(node.id, str) else node.id,\n",
    "        type=node.type.capitalize(),\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> Relationship:\n",
    "    \"\"\"\n",
    "    Map the KnowledgeGraph Relationship to the base Relationship with proper formatting.\n",
    "    \"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return Relationship(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        type=rel.type,\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "# =============================\n",
    "# **6. Define OpenAI Extraction Chain**\n",
    "# =============================\n",
    "\n",
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def get_extraction_chain(\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create a structured output chain for knowledge graph extraction using OpenAI's GPT-4.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\n",
    "          \"system\",\n",
    "          f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"Person\"**. Avoid using more specific terms like \"Mathematician\" or \"Scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "{'- **Allowed Node Labels:** ' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "{'- **Allowed Relationship Types:** ' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "## 5. Handling Unrelated Questions\n",
    "- **Recognizing Irrelevance**: If the question does not pertain to the provided schema or cannot be answered using the knowledge graph, respond with a clear and concise message indicating the lack of context.\n",
    "- **Example Response:** \"I don't have the context to answer that question.\"\n",
    "## 6. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "          \"\"\"\n",
    "        ),\n",
    "            (\"user\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "            (\"user\", \"Tip: Make sure to answer in the correct format\"),\n",
    "        ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# **7. Define Function to Extract and Store Graph**\n",
    "# =============================\n",
    "\n",
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ) -> GraphDocument:\n",
    "    \"\"\"\n",
    "    Extracts a knowledge graph from the given document and stores it in Neo4j.\n",
    "    \n",
    "    Args:\n",
    "        document (Document): The document to extract the knowledge graph from.\n",
    "        allowed_nodes (Optional[List[str]]): List of allowed node labels.\n",
    "        allowed_rels (Optional[List[str]]): List of allowed relationship types.\n",
    "        \n",
    "    Returns:\n",
    "        GraphDocument: The extracted knowledge graph document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract graph data using OpenAI functions\n",
    "        extract_chain = get_extraction_chain(allowed_nodes, allowed_rels)\n",
    "        data = extract_chain.invoke(document.page_content)['function']\n",
    "        \n",
    "        # Construct a GraphDocument\n",
    "        graph_document = GraphDocument(\n",
    "            nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "            relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "            source = document\n",
    "        )\n",
    "        \n",
    "        # Store information into Neo4j\n",
    "        print(f\"Storing GraphDocument with {len(graph_document.nodes)} nodes and {len(graph_document.relationships)} relationships.\")\n",
    "        graph.add_graph_documents([graph_document])\n",
    "        \n",
    "        return graph_document\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting and storing graph: {e}\")\n",
    "        raise e\n",
    "\n",
    "# =============================\n",
    "# **8. Initialize Neo4j Graph**\n",
    "# =============================\n",
    "\n",
    "# Initialize Neo4jGraph\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# **9. Load PDF and Split into Documents**\n",
    "# =============================\n",
    "\n",
    "# Initialize PDF Loader\n",
    "loader = PyPDFLoader(\"/Users/ishukalra/Documents/kaya/nlp-hands-on-projects/data/Lyft-Annual-Report-2021.pdf\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Load and split the PDF into pages\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "# Only take the first 4 pages of the document\n",
    "documents = text_splitter.split_documents(pages[:4])\n",
    "\n",
    "print(f\"Number of documents after splitting: {len(documents)}\")\n",
    "\n",
    "# =============================\n",
    "# **10. Iterate Over Documents and Extract Graph**\n",
    "# =============================\n",
    "\n",
    "distinct_nodes = set()\n",
    "relations = []\n",
    "\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents), desc=\"Processing Documents\"):\n",
    "    try:\n",
    "        graph_document = extract_and_store_graph(\n",
    "            d,\n",
    "            allowed_nodes=['Person', 'Organization', 'Event', 'Location'],  # Example allowed nodes\n",
    "            allowed_rels=['WORKS_AT', 'PART_OF', 'LOCATED_IN']  # Example allowed relationships\n",
    "        )\n",
    "        \n",
    "        # Get distinct nodes\n",
    "        for node in graph_document.nodes:\n",
    "            distinct_nodes.add(node.id)\n",
    "        \n",
    "        # Get all relations   \n",
    "        for relation in graph_document.relationships:\n",
    "            relations.append(relation.type)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {i}: {e}\")\n",
    "\n",
    "# =============================\n",
    "# **11. Collect Metadata and Update JSON File**\n",
    "# =============================\n",
    "\n",
    "end_time = datetime.now() \n",
    "\n",
    "LLM = \"OpenAI-GPT-4\"\n",
    "file = loader.file_path\n",
    "processed_time = str(end_time - start_time)\n",
    "\n",
    "llm_data = {\n",
    "    \"LLM\": LLM, \n",
    "    \"File\": loader.file_path, \n",
    "    \"Processing Time\": processed_time, \n",
    "    \"Node count\": len(distinct_nodes), \n",
    "    \"Relation count\": len(relations),\n",
    "    \"Nodes\": list(distinct_nodes),\n",
    "    \"Relations\": relations\n",
    "}\n",
    "\n",
    "json_file_path = '../data/llm_comparision.json'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(json_file_path), exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Load existing data\n",
    "    if os.path.exists(json_file_path):\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            try:\n",
    "                data = json.load(json_file)\n",
    "                if isinstance(data, dict):\n",
    "                    data = [data]\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: The file {json_file_path} is corrupted. Overwriting with new data.\")\n",
    "                data = []\n",
    "    else:\n",
    "        data = []\n",
    "    \n",
    "    # Append new data\n",
    "    data.append(llm_data)   \n",
    "    \n",
    "    # Write back to JSON file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4) \n",
    "    \n",
    "    print(\"Metadata updated successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to update metadata JSON file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7338a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Cypher Query:\n",
      "CREATE VECTOR INDEX `KG_Enhanced_QnA_Biomedical` FOR (m:Chunk) ON (m.embedding) OPTIONS { indexConfig: { `vector.dimensions`: 1536, `vector.similarity_function`: \"cosine\" } }\n",
      "Vector index `KG_Enhanced_QnA_Biomedical` created successfully.\n",
      "Neo4jVector initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================\n",
    "# **12. Initialize OpenAI Embeddings and Neo4jVector**\n",
    "# =============================\n",
    "\n",
    "# Manually create the vector index with correct syntax\n",
    "def create_vector_index(uri, user, password, index_name, embedding_dimension, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Creates a vector index in Neo4j with the specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        uri (str): Neo4j URI.\n",
    "        user (str): Neo4j username.\n",
    "        password (str): Neo4j password.\n",
    "        index_name (str): Name of the vector index.\n",
    "        embedding_dimension (int): Dimension of the embedding vectors.\n",
    "        similarity_metric (str): Similarity metric ('cosine', 'euclidean', or 'dot_product').\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        try:\n",
    "            # Corrected Cypher query without IF NOT EXISTS\n",
    "            cypher_query = (\n",
    "                f\"CREATE VECTOR INDEX `{index_name}` \"\n",
    "                f\"FOR (m:Chunk) \"\n",
    "                f\"ON (m.embedding) \"\n",
    "                f\"OPTIONS {{ \"\n",
    "                f\"indexConfig: {{ \"\n",
    "                f\"`vector.dimensions`: {embedding_dimension}, \"\n",
    "                f'`vector.similarity_function`: \"{similarity_metric}\" '\n",
    "                f\"}} \"\n",
    "                f\"}}\"\n",
    "            )\n",
    "            print(\"Executing Cypher Query:\")\n",
    "            print(cypher_query)\n",
    "            session.run(cypher_query)\n",
    "            print(f\"Vector index `{index_name}` created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating vector index `{index_name}`: {e}\")\n",
    "            raise e\n",
    "        finally:\n",
    "            driver.close()\n",
    "\n",
    "# Create the vector index manually\n",
    "create_vector_index(\n",
    "    uri=NEO4J_URI,\n",
    "    user=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name='KG_Enhanced_QnA_Biomedical',  # Using underscores instead of hyphens\n",
    "    embedding_dimension=1536,  # Hardcoded as per your requirement\n",
    "    similarity_metric='cosine'  # Choose the appropriate similarity metric\n",
    ")\n",
    "\n",
    "# Now, initialize Neo4jVector without attempting to create the index again\n",
    "try:\n",
    "    neo4j_vector = Neo4jVector.from_documents(\n",
    "        documents,\n",
    "        embeddings,\n",
    "        index_name='KG_Enhanced_QnA_Biomedical',  # Using underscores\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_USER,\n",
    "        password=NEO4J_PASSWORD\n",
    "    )\n",
    "    print(\"Neo4jVector initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Neo4jVector: {e}\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138db51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "\n",
      " \n",
      "Delaware 20-8809830\n",
      "(State or other jurisdiction of\n",
      "incorporation or organization)(I.R.S. Employer\n",
      "Identification No.)\n",
      "185 Berry Street,  Suite 5000\n",
      "San Francisco , California 94107\n",
      "(Address of principal executive offices) (Zip Code)\n",
      "Registrant’s telephone number, including area code: ( 844) 250-2773  \n",
      "Securities registered pursuant to Section 12(b) of the Act: \n",
      "Title of each classTrading\n",
      "Symbol(s) Name of each exchange on which registered\n",
      "Class A common stock, par value of $0.00001 per share LYFT Nasdaq Global Select Market\n",
      "Securities registered pursuant to Section 12(g) of the Act: None \n",
      "Indicate by check mark if the Registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act. Yes ☒ No ☐\n",
      "\n",
      "Result 2:\n",
      "UNITED STATES \n",
      "SECURITIES AND EXCHANGE COMMISSION \n",
      "Washington, D.C. 20549 \n",
      "FORM 10-K /A \n",
      "(Amendment No. 1)\n",
      " \n",
      "(Mark One)\n",
      "☒ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended December 31, 2021  \n",
      "OR\n",
      "☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934 FOR THE \n",
      "TRANSITION PERIOD FROM                      TO\n",
      "Commission File Number 001-38846\n",
      "Lyft, Inc.\n",
      "(Exact name of Registrant as specified in its Charter)\n",
      " \n",
      "Delaware 20-8809830\n",
      "(State or other jurisdiction of\n",
      "incorpor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================\n",
    "# **13. Perform Similarity Search**\n",
    "# =============================\n",
    "\n",
    "query = \"Hi, tell me about the Lyft revenue\"\n",
    "vector_results = neo4j_vector.similarity_search(query, k=2)\n",
    "\n",
    "for i, res in enumerate(vector_results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(res.page_content)\n",
    "    if i != len(vector_results) - 1:\n",
    "        print()\n",
    "\n",
    "if vector_results:\n",
    "    vector_result = vector_results[0].page_content\n",
    "else:\n",
    "    print(\"No vector results found.\")\n",
    "    vector_result = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new Neo4jVectorChainCustom chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "QA Result:\n",
      "I'm sorry, but the provided context does not contain information on how to enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.base import Chain\n",
    "\n",
    "# =============================\n",
    "# **14. Define and Run QA Chain**\n",
    "# =============================\n",
    "\n",
    "# Define the Cypher query for vector search\n",
    "vector_search = \"\"\"\n",
    "WITH $embedding AS e\n",
    "CALL db.index.vector.queryNodes('KG_Enhanced_QnA_Biomedical', $k, e) YIELD node, score\n",
    "RETURN node.text AS result\n",
    "ORDER BY score DESC\n",
    "LIMIT $k\n",
    "\"\"\"\n",
    "\n",
    "# Define the custom Neo4jVectorChain\n",
    "class Neo4jVectorChainCustom(Chain):\n",
    "    graph: Neo4jGraph\n",
    "    embeddings: OpenAIEmbeddings\n",
    "    qa_chain: LLMChain\n",
    "    input_key: str = \"query\"\n",
    "    output_key: str = \"result\"\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [self.input_key]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str], run_manager=None) -> Dict[str, Any]:\n",
    "        question = inputs[self.input_key]\n",
    "        embedding = self.embeddings.embed_query(question)\n",
    "\n",
    "        context = self.graph.query(\n",
    "            vector_search,\n",
    "            {'embedding': embedding, 'k': 3}\n",
    "        )\n",
    "        context = [el['result'] for el in context]\n",
    "\n",
    "        result = self.qa_chain({\"question\": question, \"context\": context})\n",
    "        final_result = result[self.qa_chain.output_key]\n",
    "        return {self.output_key: final_result}\n",
    "\n",
    "# Initialize the custom chain\n",
    "chain = Neo4jVectorChainCustom(\n",
    "    graph=graph,\n",
    "    embeddings=embeddings,\n",
    "    qa_chain=LLMChain(llm=llm, prompt=CHAT_PROMPT),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the custom chain for QA\n",
    "question = \"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\"\n",
    "graph_result = chain.run(question)\n",
    "\n",
    "print(\"QA Result:\")\n",
    "print(graph_result)\n",
    "\n",
    "# =============================\n",
    "# **15. Alternative: Using GraphCypherQAChain**\n",
    "# =============================\n",
    "\n",
    "# chain_alternative = GraphCypherQAChain.from_llm(\n",
    "#     cypher_llm=llm,\n",
    "#     qa_llm=llm,\n",
    "#     graph=graph,\n",
    "#     verbose=True,\n",
    "#     return_intermediate_steps=True,\n",
    "#     validate_cypher=True,\n",
    "#     allow_dangerous_requests=True\n",
    "# )\n",
    "\n",
    "# graph_result_alternative = chain_alternative.run(question)\n",
    "\n",
    "# print(\"Alternative QA Result:\")\n",
    "# print(graph_result_alternative)\n",
    "\n",
    "# =============================\n",
    "# **16. Final Clean-Up**\n",
    "# =============================\n",
    "\n",
    "# Close Neo4j driver if not already closed\n",
    "# (Assuming Neo4jGraph manages the driver internally)\n",
    "# If not, ensure to close it appropriately.\n",
    "# graph.driver.close()  # Uncomment if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1100fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.24.0\n"
     ]
    }
   ],
   "source": [
    "!neo4j --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (llama-index-recursive)",
   "language": "python",
   "name": "llama-index-recursive"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
