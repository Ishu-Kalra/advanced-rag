{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages (ensure these are installed in your environment)\n",
    "# %pip install -qU pypdf==4.0.1 langchain_community\n",
    "# %pip install -U duckduckgo-search\n",
    "# %pip install -qU langchain-openai\n",
    "# %pip install faiss-cpu\n",
    "# %pip install langchain==0.3.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U sentence-transformers\n",
    "# %pip install -U torch  # For CPU\n",
    "# If you have a GPU and want to leverage it, install the appropriate PyTorch version:\n",
    "# pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import DuckDuckGoSearchResults, StructuredTool, Tool\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from functools import partial\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import DuckDuckGoSearchResults, StructuredTool, Tool\n",
    "from langchain.chat_models import ChatOpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import langchain\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg[binary,pool] in /opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages (3.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages (from psycopg[binary,pool]) (4.12.2)\n",
      "Requirement already satisfied: psycopg-binary==3.2.3 in /opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages (from psycopg[binary,pool]) (3.2.3)\n",
      "Requirement already satisfied: psycopg-pool in /opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages (from psycopg[binary,pool]) (3.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"psycopg[binary,pool]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "connection_string = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "# db = PGVector.from_documents(documents, model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securely set your OpenAI API key as an environment variable before running the script\n",
    "# Example (in your terminal):\n",
    "# export OPENAI_API_KEY=\"your-api-key\"\n",
    "\n",
    "import os\n",
    "api_key = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"API_KEY_OPENAI\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to store FAISS indexes\n",
    "FAISS_INDEX_DIR = \"./faiss_indexes\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(FAISS_INDEX_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_faiss_index_from_documents(\n",
    "    year: str,\n",
    "    file_path: str,\n",
    "    embeddings: OpenAIEmbeddings,\n",
    "    text_splitter: RecursiveCharacterTextSplitter,\n",
    "    create_anyway: bool = False\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Loads a FAISS index for a year if it exists; otherwise, creates and saves a new one.\n",
    "    \"\"\"\n",
    "    index_path = os.path.join(FAISS_INDEX_DIR, f\"{year.lower()}_documents\")\n",
    "    \n",
    "    if not create_anyway and os.path.exists(index_path):\n",
    "        print(f\"Loading existing FAISS index for {year} from {index_path}\")\n",
    "        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        print(f\"Creating FAISS index for {year}\")\n",
    "        # Load and split the documents\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "        # Create FAISS vector store\n",
    "        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "        \n",
    "        # Save the FAISS index to disk\n",
    "        vectorstore.save_local(index_path)\n",
    "        print(f\"Saved FAISS index for {year} to {index_path}\")\n",
    "    \n",
    "    return vectorstore, splits\n",
    "\n",
    "def load_or_create_faiss_index_for_routing(\n",
    "    years: List[str],\n",
    "    embeddings: OpenAIEmbeddings,\n",
    "    create_anyway: bool = False\n",
    ") -> FAISS:\n",
    "    routing_index_path = os.path.join(FAISS_INDEX_DIR, \"routing\")\n",
    "    \n",
    "    if not create_anyway and os.path.exists(routing_index_path):\n",
    "        print(f\"Loading existing FAISS routing index from {routing_index_path}\")\n",
    "        routing_vectorstore = FAISS.load_local(\n",
    "            routing_index_path, embeddings, allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating FAISS routing index\")\n",
    "        routing_vectorstore = FAISS.from_texts(texts=years, embedding=embeddings)\n",
    "        routing_vectorstore.save_local(routing_index_path)\n",
    "        print(f\"Saved FAISS routing index to {routing_index_path}\")\n",
    "    \n",
    "    return routing_vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of year names and their corresponding PDF file paths\n",
    "\n",
    "years = {\n",
    "    \"UNL-Agriculture-2021\": \"../../data/unl/2021research-results.pdf\",\n",
    "    \"UNL-Agriculture-2022\": \"../../data/unl/2022research-results.pdf\",\n",
    "    \"UNL-Agriculture-2023\": \"../../data/unl/2023research-results.pdf\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to hold data for each year\n",
    "\n",
    "year_docs = {}\n",
    "year_vectorstores = {}\n",
    "year_retrievers = {}\n",
    "year_qa_tools = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/w5nd7ynn3n76b25r6j68r0g40000gn/T/ipykernel_18327/1891284508.py:9: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
      "/var/folders/lm/w5nd7ynn3n76b25r6j68r0g40000gn/T/ipykernel_18327/1891284508.py:10: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(api_key = os.environ[\"OPENAI_API_KEY\"], model='gpt-4')  # Ensure you have access to GPT-4\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Create embeddings and choose llm\n",
    "\n",
    "# llm = Ollama(model=\"llama3\", base_url=\"http://localhost:11434/\")\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "llm = ChatOpenAI(api_key = os.environ[\"OPENAI_API_KEY\"], model='gpt-4')  # Ensure you have access to GPT-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index for UNL-Agriculture-2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llama_index_recursive/lib/python3.11/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FAISS index for UNL-Agriculture-2021 to ./faiss_indexes/unl-agriculture-2021_documents\n",
      "Creating FAISS index for UNL-Agriculture-2022\n",
      "Saved FAISS index for UNL-Agriculture-2022 to ./faiss_indexes/unl-agriculture-2022_documents\n",
      "Creating FAISS index for UNL-Agriculture-2023\n",
      "Saved FAISS index for UNL-Agriculture-2023 to ./faiss_indexes/unl-agriculture-2023_documents\n"
     ]
    }
   ],
   "source": [
    "# Process each year's PDF\n",
    "# Process each year's PDF\n",
    "for year, file_path in years.items():\n",
    "    vectorstore, splits = load_or_create_faiss_index_from_documents(\n",
    "        year=year,\n",
    "        file_path=file_path,\n",
    "        embeddings=embeddings,\n",
    "        text_splitter=text_splitter,\n",
    "        create_anyway=True\n",
    "    )\n",
    "    \n",
    "    # Create a retriever for the vector store\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # Create a PGVector vector store for the year's documents\n",
    "    # vectorstore = PGVector.from_documents(\n",
    "    # documents=splits,\n",
    "    # embedding=embeddings,  # Correct parameter name\n",
    "    # connection=connection_string,\n",
    "    # collection_name=f\"{year.lower()}_documents\"  # Separate table for each year\n",
    "    # )\n",
    "    \n",
    "    # Create a retriever for the vector store\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # Store in dictionaries\n",
    "    year_vectorstores[year] = vectorstore\n",
    "    year_retrievers[year] = retriever\n",
    "    year_docs[year] = splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Revenue Growth Calculator Tool\n",
    "class RevenueGrowthInput(BaseModel):\n",
    "    revenue_new: float = Field(..., description=\"Revenue in the new year\")\n",
    "    revenue_old: float = Field(..., description=\"Revenue in the old year\")\n",
    "\n",
    "def revenue_growth_calculator(revenue_new: float, revenue_old: float) -> float:\n",
    "    \"\"\"Calculates revenue growth from the old year to the new year.\"\"\"\n",
    "    try:\n",
    "        growth = ((revenue_new - revenue_old) / revenue_old) * 100\n",
    "        print(f\"Calculated revenue growth: {growth}%\")\n",
    "        return growth\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Division by zero encountered in revenue growth calculation.\")\n",
    "        return 0.0\n",
    "\n",
    "revenue_growth_tool = StructuredTool.from_function(\n",
    "    func=revenue_growth_calculator,\n",
    "    name=\"Revenue Growth Calculator\",\n",
    "    description=(\n",
    "        \"Calculates the revenue growth percentage between two years. \"\n",
    "        \"Requires the following inputs in a JSON object:\\n\"\n",
    "        \"- 'revenue_new': float - Revenue in the new year\\n\"\n",
    "        \"- 'revenue_old': float - Revenue in the old year\"\n",
    "    ),\n",
    "    args_schema=RevenueGrowthInput\n",
    ")\n",
    "\n",
    "# Define the DuckDuckGo search tool\n",
    "web_search_tool = DuckDuckGoSearchResults(\n",
    "    name=\"Web Search\",\n",
    "    description=(\n",
    "        \"Use this tool to search the web for information not present in the given documents\"\n",
    "        # \"if they are not found in the documents.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS routing index\n",
      "Saved FAISS routing index to ./faiss_indexes/routing\n",
      "Setting up QA tool for UNL-Agriculture-2021\n",
      "QA tool for UNL-Agriculture-2021 created\n",
      "Setting up QA tool for UNL-Agriculture-2022\n",
      "QA tool for UNL-Agriculture-2022 created\n",
      "Setting up QA tool for UNL-Agriculture-2023\n",
      "QA tool for UNL-Agriculture-2023 created\n"
     ]
    }
   ],
   "source": [
    "# List of year names for routing\n",
    "year_names = list(years.keys())\n",
    "\n",
    "# Create embeddings for routing (reuse the existing embeddings)\n",
    "routing_embeddings = embeddings  # Using the same embedding model\n",
    "\n",
    "# Initialize FAISS vector store for routing using the utility function\n",
    "routing_vectorstore = load_or_create_faiss_index_for_routing(\n",
    "    years=list(years.keys()),\n",
    "    embeddings=embeddings,\n",
    "    create_anyway=True  # Force recreation of the routing index\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to get the most similar year based on query using FAISS\n",
    "def get_most_similar_year(query: str) -> Optional[str]:\n",
    "    try:\n",
    "        similar_docs = routing_vectorstore.similarity_search(query, k=1)\n",
    "        if similar_docs:\n",
    "            year = similar_docs[0].page_content\n",
    "            print(f\"Most similar year for query '{query}': {year}\")\n",
    "            return year\n",
    "        else:\n",
    "            print(f\"No similar year found for query '{query}'\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define separate Pydantic models\n",
    "class RouterInput(BaseModel):\n",
    "    query: str = Field(..., description=\"The query to determine the relevant year.\")\n",
    "\n",
    "class QAInput(BaseModel):\n",
    "    query: str = Field(..., description=\"The year for the QA tool.\")\n",
    "\n",
    "# Define the Semantic Router function\n",
    "def semantic_router(query: str) -> str:\n",
    "    year = get_most_similar_year(query)\n",
    "    if year and year in years:\n",
    "        return year\n",
    "    else:\n",
    "        return \"No Matching Year\"  # Or handle as per your requirement\n",
    "\n",
    "\n",
    "# Define the Semantic Router Tool as a StructuredTool\n",
    "router_tool = StructuredTool.from_function(\n",
    "    func=semantic_router,  # Now accepts 'query' directly\n",
    "    name=\"Semantic Router\",\n",
    "    description=(\n",
    "        \"Determines which year's knowledge base to use for answering the question. \"\n",
    "        \"Requires 'query' as input and returns the year (2021, 2022, 2023) or 'Web Search' if no match is found.\"\n",
    "    ),\n",
    "    args_schema=RouterInput  # Corrected schema\n",
    ")\n",
    "\n",
    "# Define a generic QA run function\n",
    "def run_qa(chain: RetrievalQA, query: str) -> str:\n",
    "    try:\n",
    "        response = chain.run(query)\n",
    "        print(f\"QA response: {response}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during QA run: {e}\")\n",
    "        return \"I'm sorry, I encountered an error while processing your request.\"\n",
    "\n",
    "# Create StructuredTool instances for each year\n",
    "qa_tools = []\n",
    "for year, retriever in year_retrievers.items():\n",
    "    print(f\"Setting up QA tool for {year}\")\n",
    "    \n",
    "    # Initialize the RetrievalQA chain for the year\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # Options: 'stuff', 'map_reduce', etc.\n",
    "        retriever=retriever\n",
    "    )\n",
    "    \n",
    "    # Bind the current qa_chain to the run_qa function using partial\n",
    "    bound_qa_run = partial(run_qa, chain=qa_chain)\n",
    "    \n",
    "    # Create the StructuredTool for the year\n",
    "    qa_tool = StructuredTool.from_function(\n",
    "        func=bound_qa_run,\n",
    "        name=f\"{year} QA\",\n",
    "        description=(\n",
    "            f\"Use this tool to answer questions about {year}'s information, \"\n",
    "            \"especially agricultural information.\"\n",
    "        ),\n",
    "        args_schema=QAInput  # Use the QAInput schema\n",
    "    )\n",
    "    \n",
    "    qa_tools.append(qa_tool)\n",
    "    print(f\"QA tool for {year} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update the tools list to include the router and individual QA tools\n",
    "tools = [router_tool] + qa_tools  # Remove web_search_tool if not used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the system message\n",
    "# %%\n",
    "# Define the system message with clear instructions and examples\n",
    "system_message = \"\"\"\n",
    "You are an assistant that helps answer agricultural questions based on UNL research reports.\n",
    "\n",
    "When given a question, you should:\n",
    "\n",
    "1. Use the 'Semantic Router' tool to determine which year's information is relevant to the question (UNL-Agriculture-2021, UNL-Agriculture-2022, or UNL-Agriculture-2023).\n",
    "2. Use the appropriate '{year} QA' tool to find information from the selected year's documents.\n",
    "3. When answering, consider any previous relevant queries asked by the user and mention if they are helpful.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# **Important:** When deciding to use a tool, output your response in the following JSON format **exactly**:\n",
    "\n",
    "# ```json\n",
    "# {\n",
    "#   \"action\": \"Tool Name\",\n",
    "#   \"action_input\": {\n",
    "#     // Required fields for the tool\n",
    "#   }\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_kwargs = {\n",
    "    \"system_message\": system_message\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/w5nd7ynn3n76b25r6j68r0g40000gn/T/ipykernel_18327/2149856297.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(memory_key=\"chat_history\",return_messages=True,k=7)\n",
      "/var/folders/lm/w5nd7ynn3n76b25r6j68r0g40000gn/T/ipykernel_18327/2149856297.py:6: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  agent_chain = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\",return_messages=True,k=7)\n",
    "\n",
    "# Initialize the agent with the updated tools and system message\n",
    "agent_chain = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    agent_kwargs=agent_kwargs,\n",
    "    memory = memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat_history': []}\n"
     ]
    }
   ],
   "source": [
    "# You can print or log the current state of memory\n",
    "print(memory.load_memory_variables({}))  # This will show the conversation history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/w5nd7ynn3n76b25r6j68r0g40000gn/T/ipykernel_18327/2402109360.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = agent_chain.run(\"Tell me about year 2023 agriculture?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to determine which knowledge base year to use to answer this question. In this case, it seems likely that the Semantic Router will return '2023' as the query clearly mentions '2023'. However, let's use the Semantic Router to confirm.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Semantic Router\",\n",
      "  \"action_input\": {\n",
      "    \"query\": \"Tell me about year 2023 agriculture?\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0mMost similar year for query 'Tell me about year 2023 agriculture?': UNL-Agriculture-2023\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mUNL-Agriculture-2023\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAs expected, the Semantic Router tool returned 'UNL-Agriculture-2023'. This suggests that I should use the 'UNL-Agriculture-2023 QA' tool to answer the question about agriculture in 2023.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"UNL-Agriculture-2023 QA\",\n",
      "  \"action_input\": {\n",
      "    \"query\": \"Tell me about year 2023 agriculture?\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0mQA response: In the year 2023, the final year of a research project was conducted. The plan was to rotate with a warm season cover crop, but due to a drought, the warm season cover crop winter-killed, impacting the crop rotation. For the corn crop, pre-emergence herbicide was applied which contained 32 oz/ac Roundup®, 8 oz/ac dicamba, 0.5 lb/ac atrazine, and 4 oz/ac Balance Flexx® on March 16, 2023. Also, 190 lb/ac N as anhydrous ammonia was applied on March 10, 2023. The corn crop, specifically Pioneer® P1122, was no-till planted on May 14, 2023, at a seeding rate of 25,000 seeds/ac in 30” rows. Six gallons of starter fertilizer (10 -34-0) were applied in-furrow at planting. Post-emergent herbicides included 0.5 lb/ac atrazine, 30 oz/ac DiFlexx® DUO, and 32 oz/ac of Roundup ® which were applied on May 22, 2023. The average rainfall from April 1 to September 1 in 2023 was 17.16 inches. The field was planted into green cereal rye that was 4\" tall and had been grazed by cattle in March.\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn the year 2023, the final year of a research project was conducted. The plan was to rotate with a warm season cover crop, but due to a drought, the warm season cover crop winter-killed, impacting the crop rotation. For the corn crop, pre-emergence herbicide was applied which contained 32 oz/ac Roundup®, 8 oz/ac dicamba, 0.5 lb/ac atrazine, and 4 oz/ac Balance Flexx® on March 16, 2023. Also, 190 lb/ac N as anhydrous ammonia was applied on March 10, 2023. The corn crop, specifically Pioneer® P1122, was no-till planted on May 14, 2023, at a seeding rate of 25,000 seeds/ac in 30” rows. Six gallons of starter fertilizer (10 -34-0) were applied in-furrow at planting. Post-emergent herbicides included 0.5 lb/ac atrazine, 30 oz/ac DiFlexx® DUO, and 32 oz/ac of Roundup ® which were applied on May 22, 2023. The average rainfall from April 1 to September 1 in 2023 was 17.16 inches. The field was planted into green cereal rye that was 4\" tall and had been grazed by cattle in March.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe UNL-Agriculture-2023 QA tool provided a detailed summary of agricultural practices and events in 2023. It includes information about a research project, crops, herbicide applications, planting details, and weather conditions. This should adequately answer the user's query.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"In the year 2023, the final year of a research project was conducted. The plan was to rotate with a warm season cover crop, but due to a drought, the warm season cover crop winter-killed, impacting the crop rotation. For the corn crop, pre-emergence herbicide was applied which contained 32 oz/ac Roundup®, 8 oz/ac dicamba, 0.5 lb/ac atrazine, and 4 oz/ac Balance Flexx® on March 16, 2023. Also, 190 lb/ac N as anhydrous ammonia was applied on March 10, 2023. The corn crop, specifically Pioneer® P1122, was no-till planted on May 14, 2023, at a seeding rate of 25,000 seeds/ac in 30” rows. Six gallons of starter fertilizer (10 -34-0) were applied in-furrow at planting. Post-emergent herbicides included 0.5 lb/ac atrazine, 30 oz/ac DiFlexx® DUO, and 32 oz/ac of Roundup ® which were applied on May 22, 2023. The average rainfall from April 1 to September 1 in 2023 was 17.16 inches. The field was planted into green cereal rye that was 4\\\" tall and had been grazed by cattle in March.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer: In the year 2023, the final year of a research project was conducted. The plan was to rotate with a warm season cover crop, but due to a drought, the warm season cover crop winter-killed, impacting the crop rotation. For the corn crop, pre-emergence herbicide was applied which contained 32 oz/ac Roundup®, 8 oz/ac dicamba, 0.5 lb/ac atrazine, and 4 oz/ac Balance Flexx® on March 16, 2023. Also, 190 lb/ac N as anhydrous ammonia was applied on March 10, 2023. The corn crop, specifically Pioneer® P1122, was no-till planted on May 14, 2023, at a seeding rate of 25,000 seeds/ac in 30” rows. Six gallons of starter fertilizer (10 -34-0) were applied in-furrow at planting. Post-emergent herbicides included 0.5 lb/ac atrazine, 30 oz/ac DiFlexx® DUO, and 32 oz/ac of Roundup ® which were applied on May 22, 2023. The average rainfall from April 1 to September 1 in 2023 was 17.16 inches. The field was planted into green cereal rye that was 4\" tall and had been grazed by cattle in March.\n"
     ]
    }
   ],
   "source": [
    "# Test if the memory is working by asking multiple queries\n",
    "results = agent_chain.run(\"Tell me about year 2023 agriculture?\")\n",
    "print(f\"Answer: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_recursive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
